{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Hypergraph Message Passing Neural Network (HMPNN)\n",
    "\n",
    "In this notebook, we will create and train a Hypergraph Message Passing Neural Network in the hypergraph domain. This method is introduced in the paper [Message Passing Neural Networks for\n",
    "Hypergraphs](https://arxiv.org/abs/2203.16995) by Heydari et Livi 2022. We will use a benchmark dataset, Cora, a collection of 2708 academic papers and 5429 citation relations, to do the task of node classification. There are 7 category labels, namely `Case_Based`, `Genetic_Algorithms`, `Neural_Networks`, `Probabilistic_Methods`, `Reinforcement_Learning`, `Rule_Learning` and `Theory`.\n",
    "\n",
    "Each document is initially represented as a binary vector of length 1433, standing for a unique subset of the words within the papers, in which a value of 1 means the presence of its corresponding word in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:51.222779223Z",
     "start_time": "2023-06-01T16:14:49.575421023Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric.datasets as geom_datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from topomodelx.nn.hypergraph.hmpnn import HMPNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GPU's are available, we will make use of them. Otherwise, this will run on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:51.959770754Z",
     "start_time": "2023-06-01T16:14:51.956096841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "Here we download the dataset. It contains initial representation of nodes, the adjacency information, category labels and train-val-test masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cora = geom_datasets.Planetoid(root=\"/TopoModelX/data/cora\", name=\"Cora\")\n",
    "data = cora.data\n",
    "\n",
    "x_0s = data.x\n",
    "y = data.y\n",
    "edge_index = data.edge_index\n",
    "\n",
    "train_mask = data.train_mask\n",
    "val_mask = data.val_mask\n",
    "test_mask = data.test_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neighborhood structures and lift into hypergraph domain. ##\n",
    "\n",
    "Now we retrieve the neighborhood structure (i.e. their representative matrice) that we will use to send messges from node to hyperedges. In the case of this architecture, we need the boundary matrix (or incidence matrix) $B_1$ with shape $n_\\text{nodes} \\times n_\\text{edges}$.\n",
    "\n",
    "In citation Cora dataset we lift graph structure to the hypergraph domain by creating hyperedges from 1-hop graph neighbourhood of each node. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the graph is undirected (optional but often useful for one-hop neighborhoods).\n",
    "edge_index = to_undirected(edge_index)\n",
    "\n",
    "# Create a list of one-hop neighborhoods for each node.\n",
    "one_hop_neighborhoods = []\n",
    "for node in range(data.num_nodes):\n",
    "    # Get the one-hop neighbors of the current node.\n",
    "    neighbors = data.edge_index[1, data.edge_index[0] == node]\n",
    "\n",
    "    # Append the neighbors to the list of one-hop neighborhoods.\n",
    "    one_hop_neighborhoods.append(neighbors.numpy())\n",
    "\n",
    "# Detect and eliminate duplicate hyperedges.\n",
    "unique_hyperedges = set()\n",
    "hyperedges = []\n",
    "for neighborhood in one_hop_neighborhoods:\n",
    "    # Sort the neighborhood to ensure consistent comparison.\n",
    "    neighborhood = tuple(sorted(neighborhood))\n",
    "    if neighborhood not in unique_hyperedges:\n",
    "        hyperedges.append(list(neighborhood))\n",
    "        unique_hyperedges.add(neighborhood)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally we print the statictis associated with obtained incidence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate hyperedge statistics.\n",
    "hyperedge_sizes = [len(he) for he in hyperedges]\n",
    "min_size = min(hyperedge_sizes)\n",
    "max_size = max(hyperedge_sizes)\n",
    "mean_size = np.mean(hyperedge_sizes)\n",
    "median_size = np.median(hyperedge_sizes)\n",
    "std_size = np.std(hyperedge_sizes)\n",
    "num_single_node_hyperedges = sum(np.array(hyperedge_sizes) == 1)\n",
    "\n",
    "# Print the hyperedge statistics.\n",
    "print(f'Hyperedge statistics: ')\n",
    "print('Number of hyperedges without duplicated hyperedges', len(hyperedges))\n",
    "print(f'min = {min_size}, ')\n",
    "print(f'max = {max_size}, ')\n",
    "print(f'mean = {mean_size}, ')\n",
    "print(f'median = {median_size}, ')\n",
    "print(f'std = {std_size}, ')\n",
    "print(f'Number of hyperedges with size equal to one = {num_single_node_hyperedges}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct incidence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_edges = len(hyperedges)\n",
    "incidence_1 = np.zeros((x_0s.shape[0], max_edges))\n",
    "for col, neighibourhood in enumerate(hyperedges):\n",
    "    for row in neighibourhood:\n",
    "        incidence_1[row, col] = 1\n",
    "\n",
    "assert all(incidence_1.sum(0)>0) == True, \"Some hyperedges are empty\"\n",
    "assert all(incidence_1.sum(1)>0) == True, \"Some nodes are not in any hyperedges\"\n",
    "incidence_1 = torch.Tensor(incidence_1).to_sparse_coo()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We then specify the hyperparameters and construct the model, the loss and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:58.153514385Z",
     "start_time": "2023-06-01T16:14:57.243596119Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(41)\n",
    "\n",
    "in_features = 1433\n",
    "hidden_features = 8\n",
    "num_classes = 7\n",
    "n_layers = 1\n",
    "\n",
    "model = HMPNN(in_features, (256, hidden_features), num_classes, n_layers).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "train_mask = dataset[\"train_mask\"]\n",
    "val_mask = dataset[\"val_mask\"]\n",
    "test_mask = dataset[\"test_mask\"]\n",
    "\n",
    "\n",
    "x = dataset[\"x\"]\n",
    "y = dataset[\"y\"]\n",
    "incidence_1 = dataset[\"incidence_1\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to train the model, looping over the network for a low amount of epochs. We keep training minimal for the purpose of rapid testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:15:01.683216142Z",
     "start_time": "2023-06-01T16:15:00.727075750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 train loss: 1.7054 train acc: 0.63 val loss: 1.8855 val acc: 0.34test loss: 0.3410 val acc: 0.34\n",
      "Epoch: 11 train loss: 1.6260 train acc: 0.69 val loss: 1.8205 val acc: 0.32test loss: 0.3380 val acc: 0.34\n",
      "Epoch: 16 train loss: 1.5252 train acc: 0.79 val loss: 1.7742 val acc: 0.35test loss: 0.3920 val acc: 0.39\n",
      "Epoch: 21 train loss: 1.4274 train acc: 0.85 val loss: 1.7526 val acc: 0.35test loss: 0.3850 val acc: 0.39\n",
      "Epoch: 26 train loss: 1.3159 train acc: 0.83 val loss: 1.7416 val acc: 0.36test loss: 0.3850 val acc: 0.39\n",
      "Epoch: 31 train loss: 1.2421 train acc: 0.84 val loss: 1.6929 val acc: 0.39test loss: 0.4150 val acc: 0.41\n",
      "Epoch: 36 train loss: 1.1510 train acc: 0.91 val loss: 1.6686 val acc: 0.41test loss: 0.4340 val acc: 0.43\n",
      "Epoch: 41 train loss: 1.0704 train acc: 0.91 val loss: 1.5701 val acc: 0.49test loss: 0.4900 val acc: 0.49\n",
      "Epoch: 46 train loss: 0.9775 train acc: 0.90 val loss: 1.5366 val acc: 0.50test loss: 0.5130 val acc: 0.51\n",
      "Epoch: 51 train loss: 0.9132 train acc: 0.90 val loss: 1.4844 val acc: 0.53test loss: 0.5420 val acc: 0.54\n",
      "Epoch: 56 train loss: 0.8477 train acc: 0.96 val loss: 1.4231 val acc: 0.55test loss: 0.5600 val acc: 0.56\n",
      "Epoch: 61 train loss: 0.7438 train acc: 0.97 val loss: 1.3804 val acc: 0.56test loss: 0.5830 val acc: 0.58\n",
      "Epoch: 66 train loss: 0.7040 train acc: 0.98 val loss: 1.3221 val acc: 0.59test loss: 0.6090 val acc: 0.61\n",
      "Epoch: 71 train loss: 0.6370 train acc: 0.98 val loss: 1.3007 val acc: 0.59test loss: 0.6100 val acc: 0.61\n",
      "Epoch: 76 train loss: 0.5892 train acc: 0.98 val loss: 1.2758 val acc: 0.62test loss: 0.6180 val acc: 0.62\n",
      "Epoch: 81 train loss: 0.5556 train acc: 0.99 val loss: 1.2795 val acc: 0.59test loss: 0.6190 val acc: 0.62\n",
      "Epoch: 86 train loss: 0.5107 train acc: 0.98 val loss: 1.2930 val acc: 0.59test loss: 0.6100 val acc: 0.61\n",
      "Epoch: 91 train loss: 0.4772 train acc: 0.99 val loss: 1.2791 val acc: 0.59test loss: 0.6110 val acc: 0.61\n",
      "Epoch: 96 train loss: 0.4572 train acc: 1.00 val loss: 1.2993 val acc: 0.59test loss: 0.6060 val acc: 0.61\n",
      "Epoch: 101 train loss: 0.3808 train acc: 1.00 val loss: 1.3041 val acc: 0.59test loss: 0.6120 val acc: 0.61\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "test_interval = 5\n",
    "num_epochs=100\n",
    "\n",
    "\n",
    "initial_x_1 = torch.zeros_like(x)\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_hat = model(x, initial_x_1, incidence_1)\n",
    "    loss = loss_fn(y_hat[train_mask], y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_loss = loss.item()\n",
    "    y_pred = y_hat.argmax(dim=-1)\n",
    "    train_acc = accuracy_score(y[train_mask], y_pred[train_mask])\n",
    "    \n",
    "    if epoch % test_interval == 0:\n",
    "        model.eval()\n",
    "        \n",
    "        y_hat = model(x, initial_x_1, incidence_1)\n",
    "        val_loss = loss_fn(y_hat[val_mask], y[val_mask]).item()\n",
    "        y_pred = y_hat.argmax(dim=-1)\n",
    "        val_acc = accuracy_score(y[val_mask], y_pred[val_mask])\n",
    "\n",
    "\n",
    "        test_loss = loss_fn(y_hat[test_mask], y[test_mask]).item()\n",
    "        y_pred = y_hat.argmax(dim=-1)\n",
    "        test_acc = accuracy_score(y[test_mask], y_pred[test_mask])\n",
    "        print(\n",
    "            f\"Epoch: {epoch + 1} train loss: {train_loss:.4f} train acc: {train_acc:.2f} \"\n",
    "            f\"val loss: {val_loss:.4f} val acc: {val_acc:.2f}\"\n",
    "            f\"test loss: {test_acc:.4f} val acc: {test_acc:.2f}\"\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
