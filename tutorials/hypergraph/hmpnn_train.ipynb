{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Hypergraph Message Passing Neural Network (HMPNN)\n",
    "\n",
    "In this notebook, we will create and train a Hypergraph Message Passing Neural Network in the hypergraph domain. This method is introduced in the paper [Message Passing Neural Networks for\n",
    "Hypergraphs](https://arxiv.org/abs/2203.16995) by Heydari et Livi 2022. We will use a benchmark dataset, Cora, a collection of 2708 academic papers and 5429 citation relations, to do the task of node classification. There are 7 category labels, namely `Case_Based`, `Genetic_Algorithms`, `Neural_Networks`, `Probabilistic_Methods`, `Reinforcement_Learning`, `Rule_Learning` and `Theory`.\n",
    "\n",
    "Each document is initially represented as a binary vector of length 1433, standing for a unique subset of the words within the papers, in which a value of 1 means the presence of its corresponding word in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:51.222779223Z",
     "start_time": "2023-06-01T16:14:49.575421023Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric.datasets as geom_datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "from topomodelx.nn.hypergraph.hmpnn import HMPNN\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GPU's are available, we will make use of them. Otherwise, this will run on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:51.959770754Z",
     "start_time": "2023-06-01T16:14:51.956096841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "Here we download the dataset. It contains initial representation of nodes, the adjacency information, category labels and train-val-test masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = geom_datasets.Planetoid(root=\"/TopoModelX/data/cora\", name=\"Cora\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we construct the incidence matrix ($B_1$) which is of shape $n_\\text{nodes} \\times n_\\text{edges}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"incidence_1\"] = torch.sparse_coo_tensor(\n",
    "    dataset[\"edge_index\"], torch.ones(dataset[\"edge_index\"].shape[1]), dtype=torch.long\n",
    ")\n",
    "dataset = dataset.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0s = dataset[\"x\"]\n",
    "y = dataset[\"y\"]\n",
    "incidence_1 = dataset[\"incidence_1\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We then specify the hyperparameters and construct the model, the loss and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    \"\"\" Network class that initializes the base model and readout layer.\n",
    "\n",
    "    Base model parameters:\n",
    "    ----------\n",
    "    Reqired:\n",
    "    in_channels : int\n",
    "        Dimension of the input features.\n",
    "    hidden_channels : int\n",
    "        Dimension of the hidden features.\n",
    "\n",
    "    Optitional:\n",
    "    **kwargs : dict\n",
    "        Additional arguments for the base model.\n",
    "    \n",
    "    Readout layer parameters:\n",
    "    ----------\n",
    "    out_channels : int\n",
    "        Dimension of the output features.\n",
    "    task_level : str\n",
    "        Level of the task. Either \"graph\" or \"node\".        \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            in_channels, \n",
    "            hidden_channels,\n",
    "            out_channels, \n",
    "            \n",
    "            task_level=\"graph\",\n",
    "            **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the model\n",
    "        self.base_model = HMPNN(\n",
    "            in_channels=in_channels,\n",
    "            hidden_channels=hidden_channels,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "\n",
    "        # Readout\n",
    "        self.linear = torch.nn.Linear(hidden_channels, out_channels)\n",
    "        self.out_pool = True if task_level == \"graph\" else False\n",
    "        \n",
    "    def forward(self, x_0, x_1, incidence_1):\n",
    "        # Base model\n",
    "        x_0, x_1 = self.base_model(x_0, x_1, incidence_1)\n",
    "\n",
    "        # Pool over all nodes in the hypergraph \n",
    "        if self.out_pool is True:\n",
    "            x = torch.max(x_0, dim=0)[0]\n",
    "        else:\n",
    "            x = x_0\n",
    "\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model hyperparameters\n",
    "in_channels = x_0s.shape[1]\n",
    "hidden_channels = 128\n",
    "n_layers=1\n",
    "\n",
    "# Readout hyperparameters\n",
    "out_channels = torch.unique(y).shape[0]\n",
    "task_level = \"graph\" if out_channels==1 else \"node\"\n",
    "\n",
    "\n",
    "model = Network(\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels=hidden_channels,\n",
    "    out_channels=out_channels, \n",
    "    n_layers=n_layers,\n",
    "    task_level=task_level,\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:58.153514385Z",
     "start_time": "2023-06-01T16:14:57.243596119Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "train_mask = dataset[\"train_mask\"]\n",
    "val_mask = dataset[\"val_mask\"]\n",
    "test_mask = dataset[\"test_mask\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to train the model, looping over the network for a low amount of epochs. We keep training minimal for the purpose of rapid testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:15:01.683216142Z",
     "start_time": "2023-06-01T16:15:00.727075750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 train loss: 1.2548 train acc: 0.82  val loss: 1.9955 val acc: 0.23 test loss: 0.2190 val acc: 0.22\n",
      "Epoch: 11 train loss: 0.8370 train acc: 1.00  val loss: 1.7554 val acc: 0.38 test loss: 0.3870 val acc: 0.39\n",
      "Epoch: 16 train loss: 0.4960 train acc: 1.00  val loss: 1.6560 val acc: 0.42 test loss: 0.4450 val acc: 0.45\n",
      "Epoch: 21 train loss: 0.2625 train acc: 1.00  val loss: 1.5750 val acc: 0.43 test loss: 0.4490 val acc: 0.45\n",
      "Epoch: 26 train loss: 0.1403 train acc: 1.00  val loss: 1.5674 val acc: 0.45 test loss: 0.4420 val acc: 0.44\n",
      "Epoch: 31 train loss: 0.0737 train acc: 1.00  val loss: 1.6039 val acc: 0.45 test loss: 0.4540 val acc: 0.45\n",
      "Epoch: 36 train loss: 0.0448 train acc: 1.00  val loss: 1.6474 val acc: 0.45 test loss: 0.4700 val acc: 0.47\n",
      "Epoch: 41 train loss: 0.0269 train acc: 1.00  val loss: 1.6880 val acc: 0.46 test loss: 0.4710 val acc: 0.47\n",
      "Epoch: 46 train loss: 0.0182 train acc: 1.00  val loss: 1.7079 val acc: 0.44 test loss: 0.4700 val acc: 0.47\n",
      "Epoch: 51 train loss: 0.0139 train acc: 1.00  val loss: 1.6682 val acc: 0.47 test loss: 0.4790 val acc: 0.48\n",
      "Epoch: 56 train loss: 0.0116 train acc: 1.00  val loss: 1.6277 val acc: 0.49 test loss: 0.4970 val acc: 0.50\n",
      "Epoch: 61 train loss: 0.0100 train acc: 1.00  val loss: 1.6208 val acc: 0.49 test loss: 0.5000 val acc: 0.50\n",
      "Epoch: 66 train loss: 0.0086 train acc: 1.00  val loss: 1.6295 val acc: 0.51 test loss: 0.5020 val acc: 0.50\n",
      "Epoch: 71 train loss: 0.0072 train acc: 1.00  val loss: 1.6361 val acc: 0.50 test loss: 0.5040 val acc: 0.50\n",
      "Epoch: 76 train loss: 0.0063 train acc: 1.00  val loss: 1.6177 val acc: 0.51 test loss: 0.5060 val acc: 0.51\n",
      "Epoch: 81 train loss: 0.0060 train acc: 1.00  val loss: 1.6049 val acc: 0.51 test loss: 0.5090 val acc: 0.51\n",
      "Epoch: 86 train loss: 0.0055 train acc: 1.00  val loss: 1.6047 val acc: 0.52 test loss: 0.5180 val acc: 0.52\n",
      "Epoch: 91 train loss: 0.0056 train acc: 1.00  val loss: 1.6063 val acc: 0.53 test loss: 0.5200 val acc: 0.52\n",
      "Epoch: 96 train loss: 0.0047 train acc: 1.00  val loss: 1.6037 val acc: 0.53 test loss: 0.5170 val acc: 0.52\n",
      "Epoch: 101 train loss: 0.0050 train acc: 1.00  val loss: 1.6076 val acc: 0.53 test loss: 0.5160 val acc: 0.52\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "test_interval = 5\n",
    "num_epochs=100\n",
    "\n",
    "\n",
    "initial_x_1 = torch.zeros_like(x_0s)\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_hat = model(x_0s, initial_x_1, incidence_1)\n",
    "    loss = loss_fn(y_hat[train_mask], y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_loss = loss.item()\n",
    "    y_pred = y_hat.argmax(dim=-1)\n",
    "    train_acc = accuracy_score(y[train_mask], y_pred[train_mask])\n",
    "    \n",
    "    if epoch % test_interval == 0:\n",
    "        model.eval()\n",
    "        \n",
    "        y_hat = model(x_0s, initial_x_1, incidence_1)\n",
    "        val_loss = loss_fn(y_hat[val_mask], y[val_mask]).item()\n",
    "        y_pred = y_hat.argmax(dim=-1)\n",
    "        val_acc = accuracy_score(y[val_mask], y_pred[val_mask])\n",
    "\n",
    "\n",
    "        test_loss = loss_fn(y_hat[test_mask], y[test_mask]).item()\n",
    "        y_pred = y_hat.argmax(dim=-1)\n",
    "        test_acc = accuracy_score(y[test_mask], y_pred[test_mask])\n",
    "        print(\n",
    "            f\"Epoch: {epoch + 1} train loss: {train_loss:.4f} train acc: {train_acc:.2f} \"\n",
    "            f\" val loss: {val_loss:.4f} val acc: {val_acc:.2f}\"\n",
    "            f\" test loss: {test_acc:.4f} val acc: {test_acc:.2f}\"\n",
    "        )\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
