{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Hypergraph Message Passing Neural Network (HMPNN)\n",
    "\n",
    "In this notebook, we will create and train a Hypergraph Message Passing Neural Network in the hypergraph domain. This method is introduced in the paper [Message Passing Neural Networks for\n",
    "Hypergraphs](https://arxiv.org/abs/2203.16995) by Heydari et Livi 2022. We will use a benchmark dataset, Cora, a collection of 2708 academic papers and 5429 citation relations, to do the task of node classification. There are 7 category labels, namely `Case_Based`, `Genetic_Algorithms`, `Neural_Networks`, `Probabilistic_Methods`, `Reinforcement_Learning`, `Rule_Learning` and `Theory`.\n",
    "\n",
    "Each document is initially represented as a binary vector of length 1433, standing for a unique subset of the words within the papers, in which a value of 1 means the presence of its corresponding word in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:51.222779223Z",
     "start_time": "2023-06-01T16:14:49.575421023Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.utils import to_undirected\n",
    "import torch_geometric.datasets as geom_datasets\n",
    "\n",
    "from topomodelx.nn.hypergraph.hmpnn import HMPNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GPU's are available, we will make use of them. Otherwise, this will run on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:51.959770754Z",
     "start_time": "2023-06-01T16:14:51.956096841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "Here we download the dataset. It contains initial representation of nodes, the adjacency information, category labels and train-val-test masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "cora = geom_datasets.Planetoid(root=\"/TopoModelX/data/cora\", name=\"Cora\")\n",
    "data = cora.data\n",
    "\n",
    "x_0s = data.x\n",
    "y = data.y\n",
    "edge_index = data.edge_index\n",
    "\n",
    "train_mask = data.train_mask\n",
    "val_mask = data.val_mask\n",
    "test_mask = data.test_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neighborhood structures and lift into hypergraph domain. ##\n",
    "\n",
    "Now we retrieve the neighborhood structure (i.e. their representative matrice) that we will use to send messges from node to hyperedges. In the case of this architecture, we need the boundary matrix (or incidence matrix) $B_1$ with shape $n_\\text{nodes} \\times n_\\text{edges}$.\n",
    "\n",
    "In citation Cora dataset we lift graph structure to the hypergraph domain by creating hyperedges from 1-hop graph neighbourhood of each node. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the graph is undirected (optional but often useful for one-hop neighborhoods).\n",
    "edge_index = to_undirected(edge_index)\n",
    "\n",
    "# Create a list of one-hop neighborhoods for each node.\n",
    "one_hop_neighborhoods = []\n",
    "for node in range(data.num_nodes):\n",
    "    # Get the one-hop neighbors of the current node.\n",
    "    neighbors = data.edge_index[1, data.edge_index[0] == node]\n",
    "\n",
    "    # Append the neighbors to the list of one-hop neighborhoods.\n",
    "    one_hop_neighborhoods.append(neighbors.numpy())\n",
    "\n",
    "# Detect and eliminate duplicate hyperedges.\n",
    "unique_hyperedges = set()\n",
    "hyperedges = []\n",
    "for neighborhood in one_hop_neighborhoods:\n",
    "    # Sort the neighborhood to ensure consistent comparison.\n",
    "    neighborhood = tuple(sorted(neighborhood))\n",
    "    if neighborhood not in unique_hyperedges:\n",
    "        hyperedges.append(list(neighborhood))\n",
    "        unique_hyperedges.add(neighborhood)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally we print the statictis associated with obtained incidence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperedge statistics: \n",
      "Number of hyperedges without duplicated hyperedges 2581\n",
      "min = 1, \n",
      "max = 168, \n",
      "mean = 4.003099573808601, \n",
      "median = 3.0, \n",
      "std = 5.327622607829558, \n",
      "Number of hyperedges with size equal to one = 412\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate hyperedge statistics.\n",
    "hyperedge_sizes = [len(he) for he in hyperedges]\n",
    "min_size = min(hyperedge_sizes)\n",
    "max_size = max(hyperedge_sizes)\n",
    "mean_size = np.mean(hyperedge_sizes)\n",
    "median_size = np.median(hyperedge_sizes)\n",
    "std_size = np.std(hyperedge_sizes)\n",
    "num_single_node_hyperedges = sum(np.array(hyperedge_sizes) == 1)\n",
    "\n",
    "# Print the hyperedge statistics.\n",
    "print(f'Hyperedge statistics: ')\n",
    "print('Number of hyperedges without duplicated hyperedges', len(hyperedges))\n",
    "print(f'min = {min_size}, ')\n",
    "print(f'max = {max_size}, ')\n",
    "print(f'mean = {mean_size}, ')\n",
    "print(f'median = {median_size}, ')\n",
    "print(f'std = {std_size}, ')\n",
    "print(f'Number of hyperedges with size equal to one = {num_single_node_hyperedges}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct incidence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_edges = len(hyperedges)\n",
    "incidence_1 = np.zeros((x_0s.shape[0], max_edges))\n",
    "for col, neighibourhood in enumerate(hyperedges):\n",
    "    for row in neighibourhood:\n",
    "        incidence_1[row, col] = 1\n",
    "\n",
    "assert all(incidence_1.sum(0)>0) == True, \"Some hyperedges are empty\"\n",
    "assert all(incidence_1.sum(1)>0) == True, \"Some nodes are not in any hyperedges\"\n",
    "incidence_1 = torch.Tensor(incidence_1).to_sparse_coo()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We then specify the hyperparameters and construct the model, the loss and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:58.153514385Z",
     "start_time": "2023-06-01T16:14:57.243596119Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# in_channels = x_0s.shape[1]\n",
    "# hidden_channels = 64\n",
    "# out_channels = torch.unique(y).shape[0]\n",
    "# task_level = \"graph\" if out_channels==1 else \"node\"\n",
    "\n",
    "\n",
    "in_features = 1433\n",
    "hidden_features = 8\n",
    "num_classes = 7\n",
    "n_layers = 2\n",
    "\n",
    "model = HMPNN(in_features, \n",
    "              (256, hidden_features), num_classes, n_layers).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Categorial cross-entropy loss\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Accuracy\n",
    "acc_fn = lambda y, y_hat: (y == y_hat).float().mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to train the model, looping over the network for a low amount of epochs. We keep training minimal for the purpose of rapid testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:15:01.683216142Z",
     "start_time": "2023-06-01T16:15:00.727075750Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 2708 but got size 2581 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/TopoModelX/tutorials/hypergraph/hmpnn_train.ipynb Cell 16\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737472616e67655f70696b65227d@ssh-remote%2Blevtel2/TopoModelX/tutorials/hypergraph/hmpnn_train.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737472616e67655f70696b65227d@ssh-remote%2Blevtel2/TopoModelX/tutorials/hypergraph/hmpnn_train.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737472616e67655f70696b65227d@ssh-remote%2Blevtel2/TopoModelX/tutorials/hypergraph/hmpnn_train.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m y_hat \u001b[39m=\u001b[39m model(x_0s, initial_x_1, incidence_1)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737472616e67655f70696b65227d@ssh-remote%2Blevtel2/TopoModelX/tutorials/hypergraph/hmpnn_train.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(y_pred_logits[train_mask], y[train_mask])\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f737472616e67655f70696b65227d@ssh-remote%2Blevtel2/TopoModelX/tutorials/hypergraph/hmpnn_train.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/TopoModelX/topomodelx/nn/hypergraph/hmpnn.py:87\u001b[0m, in \u001b[0;36mHMPNN.forward\u001b[0;34m(self, x_0, x_1, incidence_1)\u001b[0m\n\u001b[1;32m     85\u001b[0m x_1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_hidden_linear(x_1)\n\u001b[1;32m     86\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 87\u001b[0m     x_0, x_1 \u001b[39m=\u001b[39m layer(x_0, x_1, incidence_1)\n\u001b[1;32m     89\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_categories_linear(x_0)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/TopoModelX/topomodelx/nn/hypergraph/hmpnn_layer.py:224\u001b[0m, in \u001b[0;36mHMPNNLayer.forward\u001b[0;34m(self, x_0, x_1, incidence_1)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Forward computation.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \n\u001b[1;32m    205\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[39m    Output features of the hyperedges.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    221\u001b[0m node_messages_aggregated, node_messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_to_hyperedge_messenger(\n\u001b[1;32m    222\u001b[0m     x_0, incidence_1\n\u001b[1;32m    223\u001b[0m )\n\u001b[0;32m--> 224\u001b[0m hyperedge_messages_aggregated \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhyperedge_to_node_messenger(\n\u001b[1;32m    225\u001b[0m     x_1, incidence_1, node_messages\n\u001b[1;32m    226\u001b[0m )\n\u001b[1;32m    228\u001b[0m x_0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdating_func(\n\u001b[1;32m    229\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_regular_dropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_batchnorm(x_0)),\n\u001b[1;32m    230\u001b[0m     hyperedge_messages_aggregated,\n\u001b[1;32m    231\u001b[0m )\n\u001b[1;32m    232\u001b[0m x_1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdating_func(\n\u001b[1;32m    233\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_regular_dropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhyperedge_batchnorm(x_1)),\n\u001b[1;32m    234\u001b[0m     node_messages_aggregated,\n\u001b[1;32m    235\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/TopoModelX/topomodelx/nn/hypergraph/hmpnn_layer.py:74\u001b[0m, in \u001b[0;36m_HyperedgeToNodeMessenger.forward\u001b[0;34m(self, x_source, neighborhood, node_messages)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x_source, neighborhood, node_messages):\n\u001b[0;32m---> 74\u001b[0m     x_message \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmessage(x_source, neighborhood, node_messages)\n\u001b[1;32m     76\u001b[0m     neighborhood \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_dropout(neighborhood, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madjacency_dropout)\n\u001b[1;32m     77\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_index_i, source_index_j \u001b[39m=\u001b[39m neighborhood\u001b[39m.\u001b[39mindices()\n",
      "File \u001b[0;32m/TopoModelX/topomodelx/nn/hypergraph/hmpnn_layer.py:71\u001b[0m, in \u001b[0;36m_HyperedgeToNodeMessenger.message\u001b[0;34m(self, x_source, neighborhood, node_messages)\u001b[0m\n\u001b[1;32m     66\u001b[0m source_index_j, target_index_i \u001b[39m=\u001b[39m hyperedge_neighborhood\u001b[39m.\u001b[39mindices()\n\u001b[1;32m     67\u001b[0m node_messages_aggregated \u001b[39m=\u001b[39m scatter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maggr_func)(\n\u001b[1;32m     68\u001b[0m     node_messages\u001b[39m.\u001b[39mindex_select(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, source_index_j), target_index_i, \u001b[39m0\u001b[39m\n\u001b[1;32m     69\u001b[0m )\n\u001b[0;32m---> 71\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmessaging_func(x_source, node_messages_aggregated)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/TopoModelX/topomodelx/nn/hypergraph/hmpnn_layer.py:91\u001b[0m, in \u001b[0;36m_DefaultHyperedgeToNodeMessagingFunc.forward\u001b[0;34m(self, x_1, m_0)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x_1, m_0):\n\u001b[0;32m---> 91\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39msigmoid(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(torch\u001b[39m.\u001b[39;49mcat((x_1, m_0), dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 2708 but got size 2581 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "\n",
    "initial_x_1 = torch.zeros_like(x_0s)\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_hat = model(x_0s, initial_x_1, incidence_1)\n",
    "    loss = loss_fn(y_pred_logits[train_mask], y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_loss = loss.item()\n",
    "    train_acc = acc_fn(y_hat[train_mask].argmax(1), y[train_mask])\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_logits = model(dataset[\"x\"], initial_x_1, dataset[\"incidence_1\"])\n",
    "    val_loss = loss_fn(y_pred_logits[dataset[\"val_mask\"]], val_y_true).item()\n",
    "    y_pred = y_pred_logits.argmax(dim=-1)\n",
    "    val_acc = accuracy_score(val_y_true, y_pred[dataset[\"val_mask\"]])\n",
    "    print(\n",
    "        f\"Epoch: {epoch + 1} train loss: {train_loss:.4f} train acc: {train_acc:.2f} \"\n",
    "        f\"val loss: {val_loss:.4f} val acc: {val_acc:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the model against the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.3152 test acc: 0.55 \n"
     ]
    }
   ],
   "source": [
    "test_y_true = dataset[\"y\"][dataset[\"test_mask\"]]\n",
    "initial_x_1 = torch.zeros_like(dataset[\"x\"])\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_logits = model(dataset[\"x\"], initial_x_1, dataset[\"incidence_1\"])\n",
    "test_loss = loss_fn(y_pred_logits[dataset[\"test_mask\"]], test_y_true).item()\n",
    "y_pred = y_pred_logits.argmax(dim=-1)\n",
    "test_acc = accuracy_score(test_y_true, y_pred[dataset[\"test_mask\"]])\n",
    "print(f\"Test loss: {test_loss:.4f} test acc: {test_acc:.2f} \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
