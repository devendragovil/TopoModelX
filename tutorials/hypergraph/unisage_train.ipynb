{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "babbfc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.utils import to_undirected\n",
    "import torch_geometric.datasets as geom_datasets\n",
    "\n",
    "from topomodelx.nn.hypergraph.unisage import UniSAGE\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9edb4db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7251cc4",
   "metadata": {},
   "source": [
    "# Train a Uni-sage TNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35742145",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import data ##\n",
    "\n",
    "The first step is to import the dataset, Cora, a benchmark classification datase. We then lift the graph into our domain of choice, a hypergraph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9e6b0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cora = geom_datasets.Planetoid(root=\"/TopoModelX/data/cora\", name=\"Cora\")\n",
    "data = cora.data\n",
    "\n",
    "x_0s = data.x\n",
    "y = data.y\n",
    "edge_index = data.edge_index\n",
    "\n",
    "train_mask = data.train_mask\n",
    "val_mask = data.val_mask\n",
    "test_mask = data.test_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04619c6",
   "metadata": {},
   "source": [
    "## Define neighborhood structures and lift into hypergraph domain. ##\n",
    "\n",
    "Now we retrieve the neighborhood structure (i.e. their representative matrice) that we will use to send messges from node to hyperedges. In the case of this architecture, we need the boundary matrix (or incidence matrix) $B_1$ with shape $n_\\text{nodes} \\times n_\\text{edges}$.\n",
    "\n",
    "In citation Cora dataset we lift graph structure to the hypergraph domain by creating hyperedges from 1-hop graph neighbourhood of each node. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "971f9cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the graph is undirected (optional but often useful for one-hop neighborhoods).\n",
    "edge_index = to_undirected(edge_index)\n",
    "\n",
    "# Create a list of one-hop neighborhoods for each node.\n",
    "one_hop_neighborhoods = []\n",
    "for node in range(data.num_nodes):\n",
    "    # Get the one-hop neighbors of the current node.\n",
    "    neighbors = data.edge_index[1, data.edge_index[0] == node]\n",
    "\n",
    "    # Append the neighbors to the list of one-hop neighborhoods.\n",
    "    one_hop_neighborhoods.append(neighbors.numpy())\n",
    "\n",
    "# Detect and eliminate duplicate hyperedges.\n",
    "unique_hyperedges = set()\n",
    "hyperedges = []\n",
    "for neighborhood in one_hop_neighborhoods:\n",
    "    # Sort the neighborhood to ensure consistent comparison.\n",
    "    neighborhood = tuple(sorted(neighborhood))\n",
    "    if neighborhood not in unique_hyperedges:\n",
    "        hyperedges.append(list(neighborhood))\n",
    "        unique_hyperedges.add(neighborhood)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5e12b4",
   "metadata": {},
   "source": [
    "Additionally we print the statictis associated with obtained incidence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49d18ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperedge statistics: \n",
      "Number of hyperedges without duplicated hyperedges 2581\n",
      "min = 1, \n",
      "max = 168, \n",
      "mean = 4.003099573808601, \n",
      "median = 3.0, \n",
      "std = 5.327622607829558, \n",
      "Number of hyperedges with size equal to one = 412\n"
     ]
    }
   ],
   "source": [
    "# Calculate hyperedge statistics.\n",
    "hyperedge_sizes = [len(he) for he in hyperedges]\n",
    "min_size = min(hyperedge_sizes)\n",
    "max_size = max(hyperedge_sizes)\n",
    "mean_size = np.mean(hyperedge_sizes)\n",
    "median_size = np.median(hyperedge_sizes)\n",
    "std_size = np.std(hyperedge_sizes)\n",
    "num_single_node_hyperedges = sum(np.array(hyperedge_sizes) == 1)\n",
    "\n",
    "# Print the hyperedge statistics.\n",
    "print(f'Hyperedge statistics: ')\n",
    "print('Number of hyperedges without duplicated hyperedges', len(hyperedges))\n",
    "print(f'min = {min_size}, ')\n",
    "print(f'max = {max_size}, ')\n",
    "print(f'mean = {mean_size}, ')\n",
    "print(f'median = {median_size}, ')\n",
    "print(f'std = {std_size}, ')\n",
    "print(f'Number of hyperedges with size equal to one = {num_single_node_hyperedges}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8350a0",
   "metadata": {},
   "source": [
    "Construct incidence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdaa476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_edges = len(hyperedges)\n",
    "incidence_1 = np.zeros((x_0s.shape[0], max_edges))\n",
    "for col, neighibourhood in enumerate(hyperedges):\n",
    "    for row in neighibourhood:\n",
    "        incidence_1[row, col] = 1\n",
    "\n",
    "assert all(incidence_1.sum(0)>0) == True, \"Some hyperedges are empty\"\n",
    "assert all(incidence_1.sum(1)>0) == True, \"Some nodes are not in any hyperedges\"\n",
    "incidence_1 = torch.Tensor(incidence_1).to_sparse_coo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e0ce65",
   "metadata": {},
   "source": [
    "# Create the Neural Network\n",
    "\n",
    "Using the UniSAGELayer class, we create a neural network with stacked layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e133a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = x_0s.shape[1]\n",
    "hidden_channels = 128\n",
    "out_channels = torch.unique(y).shape[0]\n",
    "task_level = \"graph\" if out_channels==1 else \"node\"\n",
    "n_layers=1\n",
    "model = UniSAGE(\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels=hidden_channels,\n",
    "    out_channels=out_channels, \n",
    "    input_drop=0.2,\n",
    "    layer_drop=0.2,\n",
    "    n_layers=n_layers,\n",
    "    task_level=task_level,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8129a5f",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We specify the model, the loss, and an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89ae1cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and loss\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Categorial cross-entropy loss\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Accuracy\n",
    "acc_fn = lambda y, y_hat: (y == y_hat).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "078a11cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0s = torch.tensor(x_0s)\n",
    "x_0s, incidence_1, y = (\n",
    "            x_0s.float().to(device),\n",
    "            incidence_1.float().to(device),\n",
    "            torch.tensor(y, dtype=torch.long).to(device),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13020bc",
   "metadata": {},
   "source": [
    "The following cell performs the training, looping over the network for a low amount of epochs. We keep training minimal for the purpose of rapid testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "491ebe34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 \n",
      "Train_loss: 1.1164, acc: 0.9714\n",
      "Val_loss: 0.9324, Val_acc: 0.7820\n",
      "Test_loss: 1.0056, Test_acc: 0.7930\n",
      "Epoch: 10 \n",
      "Train_loss: 0.6460, acc: 0.9929\n",
      "Val_loss: 1.4363, Val_acc: 0.7640\n",
      "Test_loss: 1.7823, Test_acc: 0.7450\n",
      "Epoch: 15 \n",
      "Train_loss: 0.4511, acc: 1.0000\n",
      "Val_loss: 1.6066, Val_acc: 0.7520\n",
      "Test_loss: 1.7711, Test_acc: 0.7620\n",
      "Epoch: 20 \n",
      "Train_loss: 0.3402, acc: 1.0000\n",
      "Val_loss: 2.0556, Val_acc: 0.7520\n",
      "Test_loss: 2.4377, Test_acc: 0.7490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 \n",
      "Train_loss: 0.2747, acc: 1.0000\n",
      "Val_loss: 2.1278, Val_acc: 0.7460\n",
      "Test_loss: 2.4358, Test_acc: 0.7490\n",
      "Epoch: 30 \n",
      "Train_loss: 0.2295, acc: 1.0000\n",
      "Val_loss: 2.2190, Val_acc: 0.7500\n",
      "Test_loss: 2.5519, Test_acc: 0.7460\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "test_interval = 5\n",
    "num_epochs = 30\n",
    "\n",
    "epoch_loss = []\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    # Extract edge_index from sparse incidence matrix\n",
    "    y_hat = model(x_0s, incidence_1)\n",
    "    loss = loss_fn(y_hat[train_mask], y[train_mask])\n",
    "\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    epoch_loss.append(loss.item())\n",
    "\n",
    "    \n",
    "\n",
    "    if epoch_i % test_interval == 0:\n",
    "        \n",
    "        model.eval()\n",
    "        y_hat = model(x_0s, incidence_1)\n",
    "\n",
    "        loss = loss_fn(y_hat[train_mask], y[train_mask])\n",
    "        print(f\"Epoch: {epoch_i} \")\n",
    "        print(f\"Train_loss: {np.mean(epoch_loss):.4f}, acc: {acc_fn(y_hat[train_mask].argmax(1), y[train_mask]):.4f}\",flush=True)\n",
    "\n",
    "        loss = loss_fn(y_hat[val_mask], y[val_mask])\n",
    "        \n",
    "        print(f\"Val_loss: {loss:.4f}, Val_acc: {acc_fn(y_hat[val_mask].argmax(1), y[val_mask]):.4f}\", flush=True)\n",
    "\n",
    "        loss = loss_fn(y_hat[test_mask], y[test_mask])\n",
    "        print(f\"Test_loss: {loss:.4f}, Test_acc: {acc_fn(y_hat[test_mask].argmax(1), y[test_mask]):.4f}\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
