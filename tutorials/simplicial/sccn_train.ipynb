{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Simplicial Complex Convolutional Network (SCCN)\n",
    "\n",
    "We create a SCCN model a la [Yang et al : Efficient Representation Learning for Higher-Order Data with\n",
    "Simplicial Complexes (LoG 2022)](https://proceedings.mlr.press/v198/yang22a/yang22a.pdf)\n",
    "\n",
    "We train the model to perform binary node classification using the KarateClub benchmark dataset. \n",
    "\n",
    "The model operates on cells of all ranks up to some max rank $r_\\mathrm{max}$.\n",
    "The equations of one layer of this neural network are given by:\n",
    "\n",
    "游린 $\\quad m_{{y \\rightarrow x}}^{(r \\rightarrow r)} = (H_{r})_{xy} \\cdot h^{t,(r)}_y \\cdot \\Theta^{t,(r\\to r)}$,    (for $0\\leq r \\leq r_\\mathrm{max}$)\n",
    "\n",
    "游린 $\\quad m_{{y \\rightarrow x}}^{(r-1 \\rightarrow r)} = (B_{r}^T)_{xy} \\cdot h^{t,(r-1)}_y \\cdot \\Theta^{t,(r-1\\to r)}$,    (for $1\\leq r \\leq r_\\mathrm{max}$)\n",
    "\n",
    "游린 $\\quad m_{{y \\rightarrow x}}^{(r+1 \\rightarrow r)} = (B_{r+1})_{xy} \\cdot h^{t,(r+1)}_y \\cdot \\Theta^{t,(r+1\\to r)}$,    (for $0\\leq r \\leq r_\\mathrm{max}-1$)\n",
    "\n",
    "游릲 $\\quad m_{x}^{(r \\rightarrow r)}  = \\sum_{y \\in \\mathcal{L}_\\downarrow(x)\\bigcup \\mathcal{L}_\\uparrow(x)} m_{y \\rightarrow x}^{(r \\rightarrow r)}$\n",
    "\n",
    "游릲 $\\quad m_{x}^{(r-1 \\rightarrow r)}  = \\sum_{y \\in \\mathcal{B}(x)} m_{y \\rightarrow x}^{(r-1 \\rightarrow r)}$\n",
    "\n",
    "游릲 $\\quad m_{x}^{(r+1 \\rightarrow r)}  = \\sum_{y \\in \\mathcal{C}(x)} m_{y \\rightarrow x}^{(r+1 \\rightarrow r)}$\n",
    "\n",
    "游릴 $\\quad m_x^{(r)}  = m_x^{(r \\rightarrow r)} + m_x^{(r-1 \\rightarrow r)} + m_x^{(r+1 \\rightarrow r)}$\n",
    "\n",
    "游릱 $\\quad h_x^{t+1,(r)}  = \\sigma(m_x^{(r)})$\n",
    "\n",
    "Where the notations are defined in [Papillon et al : Architectures of Topological Deep Learning: A Survey of Topological Neural Networks (2023)](https://arxiv.org/abs/2304.10031)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import toponetx.datasets.graph as graph\n",
    "\n",
    "from topomodelx.nn.simplicial.sccn import SCCN\n",
    "from topomodelx.utils.sparse import from_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import dataset ##\n",
    "\n",
    "The first step is to import the Karate Club (https://www.jstor.org/stable/3629752) dataset. This is a singular graph with 34 nodes that belong to two different social groups. We will use these groups for the task of node-level binary classification.\n",
    "\n",
    "We must first lift our graph dataset into the simplicial complex domain.\n",
    "\n",
    "Since our task will be node classification, we must retrieve an input signal on the nodes. The signal will have shape $n_\\text{nodes} \\times$ in_channels, where in_channels is the dimension of each cell's feature. The feature dimension is `feat_dim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplicial Complex with shape (34, 78, 45, 11, 2) and dimension 4\n"
     ]
    }
   ],
   "source": [
    "dataset = graph.karate_club(complex_type=\"simplicial\", feat_dim=8)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neighborhood structures. ##\n",
    "\n",
    "Our implementation allows for features on cells up to an arbitrary maximum rank. In this dataset, we can use at most `max_rank = 3`, which is what we choose.\n",
    "\n",
    "We define incidence and adjacency matrices up to the max rank and put them in dictionaries indexed by the rank, as is expected by the `SCCNLayer`.\n",
    "The form of tha adjacency and incidence matrices could be chosen arbitrarily, here we follow the original formulation by Yang et al. quite closely and select the adjacencies as r-Hodge Laplacians $H_r$, summed with $2I$ (or just $I$ for $r\\in\\{0, r_\\mathrm{max}\\}$) to allow cells to pass messages to themselves. The incidence matrices are the usual boundary matrices $B_r$.\n",
    "One could additionally weight/normalize these matrices as suggested by Yang et al., but we refrain from doing this for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rank = 3  # There are features up to tetrahedron order in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The adjacency matrix H0 has shape: torch.Size([34, 34]).\n",
      "The adjacency matrix H1 has shape: torch.Size([78, 78]).\n",
      "The incidence matrix B1 has shape: torch.Size([34, 78]).\n",
      "The adjacency matrix H2 has shape: torch.Size([45, 45]).\n",
      "The incidence matrix B2 has shape: torch.Size([78, 45]).\n",
      "The adjacency matrix H3 has shape: torch.Size([11, 11]).\n",
      "The incidence matrix B3 has shape: torch.Size([45, 11]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gbg141/Documents/TopoProjectX/TopoModelX/venv_modelx/lib/python3.11/site-packages/scipy/sparse/_index.py:143: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "def sparse_to_torch(X):\n",
    "    return from_sparse(X)\n",
    "\n",
    "\n",
    "incidences = {\n",
    "    f\"rank_{r}\": sparse_to_torch(dataset.incidence_matrix(rank=r))\n",
    "    for r in range(1, max_rank + 1)\n",
    "}\n",
    "\n",
    "adjacencies = {}\n",
    "adjacencies[\"rank_0\"] = (\n",
    "    sparse_to_torch(dataset.adjacency_matrix(rank=0))\n",
    "    + torch.eye(dataset.shape[0]).to_sparse()\n",
    ")\n",
    "for r in range(1, max_rank):\n",
    "    adjacencies[f\"rank_{r}\"] = (\n",
    "        sparse_to_torch(\n",
    "            dataset.adjacency_matrix(rank=r) + dataset.coadjacency_matrix(rank=r)\n",
    "        )\n",
    "        + 2 * torch.eye(dataset.shape[r]).to_sparse()\n",
    "    )\n",
    "adjacencies[f\"rank_{max_rank}\"] = (\n",
    "    sparse_to_torch(dataset.coadjacency_matrix(rank=max_rank))\n",
    "    + torch.eye(dataset.shape[max_rank]).to_sparse()\n",
    ")\n",
    "\n",
    "for r in range(max_rank + 1):\n",
    "    print(f\"The adjacency matrix H{r} has shape: {adjacencies[f'rank_{r}'].shape}.\")\n",
    "    if r > 0:\n",
    "        print(f\"The incidence matrix B{r} has shape: {incidences[f'rank_{r}'].shape}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import signal ##\n",
    "\n",
    "We import the features at each rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = []\n",
    "for _, v in dataset.get_simplex_attributes(\"node_feat\").items():\n",
    "    x_0.append(v)\n",
    "x_0 = torch.tensor(np.stack(x_0))\n",
    "channels_nodes = x_0.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 34 nodes with features of dimension 8.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {x_0.shape[0]} nodes with features of dimension {x_0.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load edge features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = []\n",
    "for k, v in dataset.get_simplex_attributes(\"edge_feat\").items():\n",
    "    x_1.append(v)\n",
    "x_1 = torch.tensor(np.stack(x_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 78 edges with features of dimension 8.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {x_1.shape[0]} edges with features of dimension {x_1.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for face features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = []\n",
    "for k, v in dataset.get_simplex_attributes(\"face_feat\").items():\n",
    "    x_2.append(v)\n",
    "x_2 = torch.tensor(np.stack(x_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 45 faces with features of dimension 8.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {x_2.shape[0]} faces with features of dimension {x_2.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher order features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_3 = []\n",
    "for k, v in dataset.get_simplex_attributes(\"tetrahedron_feat\").items():\n",
    "    x_3.append(v)\n",
    "x_3 = torch.tensor(np.stack(x_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11 tetrahedrons with features of dimension 8.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"There are {x_3.shape[0]} tetrahedrons with features of dimension {x_3.shape[1]}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are organized in a dictionary keeping track of their rank, similar to the adjacencies/incidences earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {\"rank_0\": x_0, \"rank_1\": x_1, \"rank_2\": x_2, \"rank_3\": x_3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define binary labels\n",
    "We retrieve the labels associated to the nodes of each input simplex. In the KarateClub dataset, two social groups emerge. So we assign binary labels to the nodes indicating of which group they are a part.\n",
    "\n",
    "We keep the last four nodes' true labels for the purpose of testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(\n",
    "    [\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "    ]\n",
    ")\n",
    "\n",
    "y_train = torch.from_numpy(y[:30])\n",
    "y_test = torch.from_numpy(y[30:])\n",
    "y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Neural Network\n",
    "\n",
    "Using the HSNLayer class, we create a neural network with stacked layers. A linear layer at the end produces an output with shape $n_\\text{nodes}$, so we can compare with our binary labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We specify the model with our pre-made neighborhood structures and specify an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SCCN(\n",
    "    channels=channels_nodes,\n",
    "    max_rank=max_rank,\n",
    "    n_layers=2,\n",
    "    update_func=\"sigmoid\",\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell performs the training, looping over the network for a low number of epochs. Typically achieves 100% train accuracy. Test accuracy is more arbitrary between runs, likely due to the small dataset set size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 0.7254 Train_acc: 0.4333\n",
      "Epoch: 2 loss: 0.6657 Train_acc: 0.7667\n",
      "Epoch: 3 loss: 0.6190 Train_acc: 0.7333\n",
      "Epoch: 4 loss: 0.5810 Train_acc: 0.6667\n",
      "Epoch: 5 loss: 0.5431 Train_acc: 0.7333\n",
      "Epoch: 6 loss: 0.5013 Train_acc: 0.7333\n",
      "Epoch: 7 loss: 0.4627 Train_acc: 0.7667\n",
      "Epoch: 8 loss: 0.4161 Train_acc: 0.7667\n",
      "Epoch: 9 loss: 0.3676 Train_acc: 0.7667\n",
      "Epoch: 10 loss: 0.3208 Train_acc: 0.7667\n",
      "Test_acc: 0.2500\n",
      "Epoch: 11 loss: 0.2791 Train_acc: 0.8333\n",
      "Epoch: 12 loss: 0.2341 Train_acc: 0.9000\n",
      "Epoch: 13 loss: 0.1715 Train_acc: 0.9667\n",
      "Epoch: 14 loss: 0.1141 Train_acc: 1.0000\n",
      "Epoch: 15 loss: 0.0803 Train_acc: 1.0000\n",
      "Epoch: 16 loss: 0.0638 Train_acc: 1.0000\n",
      "Epoch: 17 loss: 0.0531 Train_acc: 1.0000\n",
      "Epoch: 18 loss: 0.0419 Train_acc: 1.0000\n",
      "Epoch: 19 loss: 0.0343 Train_acc: 1.0000\n",
      "Epoch: 20 loss: 0.0253 Train_acc: 1.0000\n",
      "Test_acc: 0.7500\n",
      "Epoch: 21 loss: 0.0195 Train_acc: 1.0000\n",
      "Epoch: 22 loss: 0.0154 Train_acc: 1.0000\n",
      "Epoch: 23 loss: 0.0122 Train_acc: 1.0000\n",
      "Epoch: 24 loss: 0.0096 Train_acc: 1.0000\n",
      "Epoch: 25 loss: 0.0077 Train_acc: 1.0000\n",
      "Epoch: 26 loss: 0.0063 Train_acc: 1.0000\n",
      "Epoch: 27 loss: 0.0052 Train_acc: 1.0000\n",
      "Epoch: 28 loss: 0.0045 Train_acc: 1.0000\n",
      "Epoch: 29 loss: 0.0039 Train_acc: 1.0000\n",
      "Epoch: 30 loss: 0.0035 Train_acc: 1.0000\n",
      "Test_acc: 0.7500\n",
      "Epoch: 31 loss: 0.0032 Train_acc: 1.0000\n",
      "Epoch: 32 loss: 0.0029 Train_acc: 1.0000\n",
      "Epoch: 33 loss: 0.0027 Train_acc: 1.0000\n",
      "Epoch: 34 loss: 0.0025 Train_acc: 1.0000\n",
      "Epoch: 35 loss: 0.0024 Train_acc: 1.0000\n",
      "Epoch: 36 loss: 0.0022 Train_acc: 1.0000\n",
      "Epoch: 37 loss: 0.0021 Train_acc: 1.0000\n",
      "Epoch: 38 loss: 0.0020 Train_acc: 1.0000\n",
      "Epoch: 39 loss: 0.0019 Train_acc: 1.0000\n",
      "Epoch: 40 loss: 0.0018 Train_acc: 1.0000\n",
      "Test_acc: 0.7500\n",
      "Epoch: 41 loss: 0.0017 Train_acc: 1.0000\n",
      "Epoch: 42 loss: 0.0017 Train_acc: 1.0000\n",
      "Epoch: 43 loss: 0.0016 Train_acc: 1.0000\n",
      "Epoch: 44 loss: 0.0016 Train_acc: 1.0000\n",
      "Epoch: 45 loss: 0.0015 Train_acc: 1.0000\n",
      "Epoch: 46 loss: 0.0015 Train_acc: 1.0000\n",
      "Epoch: 47 loss: 0.0014 Train_acc: 1.0000\n",
      "Epoch: 48 loss: 0.0014 Train_acc: 1.0000\n",
      "Epoch: 49 loss: 0.0014 Train_acc: 1.0000\n",
      "Epoch: 50 loss: 0.0013 Train_acc: 1.0000\n",
      "Test_acc: 0.7500\n",
      "Epoch: 51 loss: 0.0013 Train_acc: 1.0000\n",
      "Epoch: 52 loss: 0.0013 Train_acc: 1.0000\n",
      "Epoch: 53 loss: 0.0012 Train_acc: 1.0000\n",
      "Epoch: 54 loss: 0.0012 Train_acc: 1.0000\n",
      "Epoch: 55 loss: 0.0012 Train_acc: 1.0000\n",
      "Epoch: 56 loss: 0.0012 Train_acc: 1.0000\n",
      "Epoch: 57 loss: 0.0011 Train_acc: 1.0000\n",
      "Epoch: 58 loss: 0.0011 Train_acc: 1.0000\n",
      "Epoch: 59 loss: 0.0011 Train_acc: 1.0000\n",
      "Epoch: 60 loss: 0.0011 Train_acc: 1.0000\n",
      "Test_acc: 0.7500\n",
      "Epoch: 61 loss: 0.0011 Train_acc: 1.0000\n",
      "Epoch: 62 loss: 0.0011 Train_acc: 1.0000\n",
      "Epoch: 63 loss: 0.0010 Train_acc: 1.0000\n",
      "Epoch: 64 loss: 0.0010 Train_acc: 1.0000\n",
      "Epoch: 65 loss: 0.0010 Train_acc: 1.0000\n",
      "Epoch: 66 loss: 0.0010 Train_acc: 1.0000\n",
      "Epoch: 67 loss: 0.0010 Train_acc: 1.0000\n",
      "Epoch: 68 loss: 0.0010 Train_acc: 1.0000\n",
      "Epoch: 69 loss: 0.0010 Train_acc: 1.0000\n",
      "Epoch: 70 loss: 0.0010 Train_acc: 1.0000\n",
      "Test_acc: 0.7500\n",
      "Epoch: 71 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 72 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 73 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 74 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 75 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 76 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 77 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 78 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 79 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 80 loss: 0.0009 Train_acc: 1.0000\n",
      "Test_acc: 0.7500\n",
      "Epoch: 81 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 82 loss: 0.0008 Train_acc: 1.0000\n",
      "Epoch: 83 loss: 0.0008 Train_acc: 1.0000\n",
      "Epoch: 84 loss: 0.0008 Train_acc: 1.0000\n",
      "Epoch: 85 loss: 0.0008 Train_acc: 1.0000\n",
      "Epoch: 86 loss: 0.0008 Train_acc: 1.0000\n",
      "Epoch: 87 loss: 0.0008 Train_acc: 1.0000\n",
      "Epoch: 88 loss: 0.0008 Train_acc: 1.0000\n",
      "Epoch: 89 loss: 0.0008 Train_acc: 1.0000\n",
      "Epoch: 90 loss: 0.0008 Train_acc: 1.0000\n",
      "Test_acc: 0.7500\n",
      "Epoch: 91 loss: 0.0008 Train_acc: 1.0000\n",
      "Epoch: 92 loss: 0.0008 Train_acc: 1.0000\n",
      "Epoch: 93 loss: 0.0008 Train_acc: 1.0000\n",
      "Epoch: 94 loss: 0.0008 Train_acc: 1.0000\n",
      "Epoch: 95 loss: 0.0008 Train_acc: 1.0000\n",
      "Epoch: 96 loss: 0.0007 Train_acc: 1.0000\n",
      "Epoch: 97 loss: 0.0007 Train_acc: 1.0000\n",
      "Epoch: 98 loss: 0.0007 Train_acc: 1.0000\n",
      "Epoch: 99 loss: 0.0007 Train_acc: 1.0000\n",
      "Epoch: 100 loss: 0.0007 Train_acc: 1.0000\n",
      "Test_acc: 0.7500\n"
     ]
    }
   ],
   "source": [
    "test_interval = 10\n",
    "num_epochs = 100\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_hat = model(features, incidences, adjacencies)\n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "        y_hat[: len(y_train)].float(), y_train.float()\n",
    "    )\n",
    "    epoch_loss.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    y_pred = (y_hat > 0).long()\n",
    "    accuracy = (y_pred[: len(y_train)] == y_train).float().mean().item()\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f} Train_acc: {accuracy:.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            y_hat_test = model(features, incidences, adjacencies)\n",
    "            y_pred_test = (y_hat_test > 0).long()\n",
    "            test_accuracy = (\n",
    "                (y_pred_test[-len(y_test) :] == y_test).float().mean().item()\n",
    "            )\n",
    "            print(f\"Test_acc: {test_accuracy:.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
