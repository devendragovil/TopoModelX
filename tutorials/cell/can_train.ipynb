{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Convolutional Cell Complex Network (CCXN)\n",
    "\n",
    "We create and train a simplified version of the CCXN originally proposed in [Hajij et. al : Cell Complex Neural Networks (2020)](https://arxiv.org/pdf/2010.00743.pdf).\n",
    "\n",
    "### The Neural Network:\n",
    "\n",
    "The equations of one layer of this neural network are given by:\n",
    "\n",
    "1. A convolution from nodes to nodes using an adjacency message passing scheme (AMPS):\n",
    "\n",
    "游린 $\\quad m_{y \\rightarrow \\{z\\} \\rightarrow x}^{(0 \\rightarrow 1 \\rightarrow 0)} = M_{\\mathcal{L}_\\uparrow}^t(h_x^{t,(0)}, h_y^{t,(0)}, \\Theta^{t,(y \\rightarrow x)})$ \n",
    "\n",
    "游릲 $\\quad m_x^{(0 \\rightarrow 1 \\rightarrow 0)} = AGG_{y \\in \\mathcal{L}_\\uparrow(x)}(m_{y \\rightarrow \\{z\\} \\rightarrow x}^{0 \\rightarrow 1 \\rightarrow 0})$ \n",
    "\n",
    "游릴 $\\quad m_x^{(0)} = m_x^{(0 \\rightarrow 1 \\rightarrow 0)}$ \n",
    "\n",
    "游릱 $\\quad h_x^{t+1,(0)} = U^{t}(h_x^{t,(0)}, m_x^{(0)})$\n",
    "\n",
    "2. A convolution from edges to faces using a cohomology message passing scheme:\n",
    "\n",
    "游린 $\\quad m_{y \\rightarrow x}^{(r' \\rightarrow r)} = M^t_{\\mathcal{C}}(h_{x}^{t,(r)}, h_y^{t,(r')}, x, y)$ \n",
    "\n",
    "游릲 $\\quad m_x^{(r' \\rightarrow r)}  = AGG_{y \\in \\mathcal{C}(x)} m_{y \\rightarrow x}^{(r' \\rightarrow r)}$ \n",
    "\n",
    "游릴 $\\quad m_x^{(r)} = m_x^{(r' \\rightarrow r)}$ \n",
    "\n",
    "游릱 $\\quad h_{x}^{t+1,(r)} = U^{t,(r)}(h_{x}^{t,(r)}, m_{x}^{(r)})$\n",
    "\n",
    "Where the notations are defined in [Papillon et al : Architectures of Topological Deep Learning: A Survey of Topological Neural Networks (2023)](https://arxiv.org/abs/2304.10031).\n",
    "\n",
    "### The Task:\n",
    "\n",
    "We train this model to perform entire complex classification on [`MUTAG` from the TUDataset](https://paperswithcode.com/dataset/mutag). This dataset contains:\n",
    "- 188 samples of chemical compounds represented as graphs,\n",
    "- with 7 discrete node features.\n",
    "\n",
    "The task is to predict the mutagenicity of each compound on Salmonella typhimurium."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T09:06:36.009880829Z",
     "start_time": "2023-05-31T09:06:34.285257706Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from toponetx import CellComplex\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from topomodelx.nn.cell.can_layer import CANLayer\n",
    "from topomodelx.nn.cell.attentional_lift_layer import MultiHeadLiftLayer\n",
    "from topomodelx.nn.cell.attentional_pooling_layer import PoolLayer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GPU's are available, we will make use of them. Otherwise, this will run on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T09:13:53.006542411Z",
     "start_time": "2023-05-31T09:13:52.963074076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import data ##\n",
    "\n",
    "We import a subset of MUTAG, a benchmark dataset for graph classification. \n",
    "\n",
    "We then lift each graph into our topological domain of choice, here: a cell complex.\n",
    "\n",
    "We also retrieve:\n",
    "- input signals `x_0` and `x_1` on the nodes (0-cells) and edges (1-cells) for each complex: these will be the model's inputs,\n",
    "- a binary classification label `y` associated to the cell complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T09:13:55.279147916Z",
     "start_time": "2023-05-31T09:13:55.269057585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 40], x=[18, 7], edge_attr=[40, 4], y=[1])\n",
      "Features on nodes for the 0th cell complex: torch.Size([17, 7]).\n",
      "Features on edges for the 0th cell complex: torch.Size([38, 4]).\n",
      "Label of 0th cell complex: 1.\n"
     ]
    }
   ],
   "source": [
    "dataset = TUDataset(\n",
    "    root=\"/tmp/MUTAG\", name=\"MUTAG\", use_edge_attr=True, use_node_attr=True\n",
    ")\n",
    "dataset = dataset[:20]\n",
    "cc_list = []\n",
    "x_0_list = []\n",
    "x_1_list = []\n",
    "y_list = []\n",
    "for graph in dataset:\n",
    "    cell_complex = CellComplex(to_networkx(graph))\n",
    "    cc_list.append(cell_complex)\n",
    "    x_0_list.append(graph.x)\n",
    "    x_1_list.append(graph.edge_attr)\n",
    "    y_list.append(int(graph.y))\n",
    "else:\n",
    "    print(graph)\n",
    "\n",
    "i_cc = 0\n",
    "print(f\"Features on nodes for the {i_cc}th cell complex: {x_0_list[i_cc].shape}.\")\n",
    "print(f\"Features on edges for the {i_cc}th cell complex: {x_1_list[i_cc].shape}.\")\n",
    "print(f\"Label of {i_cc}th cell complex: {y_list[i_cc]}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neighborhood structures. ##\n",
    "\n",
    "Implementing the CCXN architecture will require to perform message passing along neighborhood structures of the cell complexes.\n",
    "\n",
    "Thus, now we retrieve these neighborhood structures (i.e. their representative matrices) that we will use to send messages. \n",
    "\n",
    "For the CCXN, we need the adjacency matrix $A_{\\uparrow, 0}$ and the coboundary matrix $B_2^T$ of each cell complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T09:13:55.832585216Z",
     "start_time": "2023-05-31T09:13:55.815448708Z"
    }
   },
   "outputs": [],
   "source": [
    "lower_neighborhood_list = []\n",
    "upper_neighborhood_list = []\n",
    "adjacency_0_list = []\n",
    "\n",
    "for cell_complex in cc_list:\n",
    "    adjacency_0 = cell_complex.adjacency_matrix(rank=0)\n",
    "    adjacency_0 = torch.from_numpy(adjacency_0.todense()).to_sparse()\n",
    "    adjacency_0_list.append(adjacency_0)\n",
    "\n",
    "    lower_neighborhood_t = cell_complex.down_laplacian_matrix(rank=1)\n",
    "    lower_neighborhood_t = torch.from_numpy(lower_neighborhood_t.todense()).to_sparse()\n",
    "    lower_neighborhood_list.append(lower_neighborhood_t)\n",
    "\n",
    "    try:\n",
    "        upper_neighborhood_t = cell_complex.up_laplacian_matrix(rank=1)\n",
    "        upper_neighborhood_t = torch.from_numpy(\n",
    "            upper_neighborhood_t.todense()\n",
    "        ).to_sparse()\n",
    "    except:\n",
    "        upper_neighborhood_t = np.zeros(\n",
    "            (lower_neighborhood_t.shape[0], lower_neighborhood_t.shape[0])\n",
    "        )\n",
    "        upper_neighborhood_t = torch.from_numpy(upper_neighborhood_t).to_sparse()\n",
    "\n",
    "    upper_neighborhood_list.append(upper_neighborhood_t)\n",
    "\n",
    "i_cc = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Neural Network\n",
    "\n",
    "Using the CCXNLayer class, we create a neural network with stacked layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T09:13:56.672913561Z",
     "start_time": "2023-05-31T09:13:56.667986426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of input features on nodes, edges and faces are: 7, 4 and 5.\n"
     ]
    }
   ],
   "source": [
    "in_channels_0 = x_0_list[0].shape[-1]\n",
    "in_channels_1 = x_1_list[0].shape[-1]\n",
    "in_channels_2 = 5\n",
    "print(\n",
    "    f\"The dimension of input features on nodes, edges and faces are: {in_channels_0}, {in_channels_1} and {in_channels_2}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T09:19:39.455212236Z",
     "start_time": "2023-05-31T09:19:39.452286461Z"
    }
   },
   "outputs": [],
   "source": [
    "class CAN(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels_0,\n",
    "        in_channels_1,\n",
    "        out_channels,\n",
    "        num_classes,\n",
    "        dropout=0.5,\n",
    "        heads=3,\n",
    "        concat=True,\n",
    "        skip_connection=True,\n",
    "        att_activation=torch.nn.LeakyReLU(0.2),\n",
    "        n_layers=2,\n",
    "        att_lift=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if att_lift:\n",
    "            self.lift_layer = MultiHeadLiftLayer(\n",
    "                in_channels_0=in_channels_0,\n",
    "                heads=in_channels_0,\n",
    "                signal_lift_dropout=0.5,\n",
    "            )\n",
    "            in_channels_1 = in_channels_1 + in_channels_0\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        layers.append(\n",
    "            CANLayer(\n",
    "                in_channels=in_channels_1,\n",
    "                out_channels=out_channels,\n",
    "                heads=heads,\n",
    "                concat=concat,\n",
    "                skip_connection=skip_connection,\n",
    "                att_activation=att_activation,\n",
    "                aggr_func=\"sum\",\n",
    "                update_func=\"relu\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        for _ in range(n_layers - 1):\n",
    "            layers.append(\n",
    "                CANLayer(\n",
    "                    in_channels=out_channels * heads,\n",
    "                    out_channels=out_channels,\n",
    "                    dropout=dropout,\n",
    "                    heads=heads,\n",
    "                    concat=concat,\n",
    "                    skip_connection=skip_connection,\n",
    "                    att_activation=att_activation,\n",
    "                    aggr_func=\"sum\",\n",
    "                    update_func=\"relu\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if _ == n_layers-2:\n",
    "                layers.append(\n",
    "                    PoolLayer(\n",
    "                        k_pool=0.5,\n",
    "                        in_channels_0=out_channels * heads,\n",
    "                        signal_pool_activation=torch.nn.Sigmoid(),\n",
    "                        readout=True,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        self.layers = torch.nn.ModuleList(layers)\n",
    "        self.lin_0 = torch.nn.Linear(heads * out_channels, 128)\n",
    "        self.lin_1 = torch.nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(\n",
    "        self, x_0, x_1, neighborhood_0_to_0, lower_neighborhood, upper_neighborhood\n",
    "    ):\n",
    "        if hasattr(self, \"lift_layer\"):\n",
    "            x_1 = self.lift_layer(x_0, neighborhood_0_to_0, x_1)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x_1 = layer(x_1, lower_neighborhood, upper_neighborhood)\n",
    "            x_1 = F.dropout(x_1, p=0.5, training=self.training)\n",
    "\n",
    "        # max pooling over all nodes in each graph\n",
    "        x = x_1.max(dim=0)[0]\n",
    "\n",
    "        # Feed-Foward Neural Network to predict the graph label\n",
    "        out = self.lin_1(torch.nn.functional.relu(self.lin_0(x)))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We specify the model, initialize loss, and specify an optimizer. We first try it without any attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T09:19:40.411845803Z",
     "start_time": "2023-05-31T09:19:40.408861921Z"
    }
   },
   "outputs": [],
   "source": [
    "model = CAN(\n",
    "    in_channels_0,\n",
    "    in_channels_1,\n",
    "    32,\n",
    "    dropout=0.5,\n",
    "    heads=3,\n",
    "    num_classes=2,\n",
    "    n_layers=2,\n",
    "    att_lift=True,\n",
    ")\n",
    "model = model.to(device)\n",
    "crit = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CAN(\n",
       "  (lift_layer): MultiHeadLiftLayer(\n",
       "    (lifts): LiftLayer()\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0): CANLayer(\n",
       "      (lower_att): MultiHeadCellAttention(\n",
       "        (att_activation): LeakyReLU(negative_slope=0.2)\n",
       "        (lin): Linear(in_features=11, out_features=96, bias=False)\n",
       "      )\n",
       "      (upper_att): MultiHeadCellAttention(\n",
       "        (att_activation): LeakyReLU(negative_slope=0.2)\n",
       "        (lin): Linear(in_features=11, out_features=96, bias=False)\n",
       "      )\n",
       "      (lin): Linear(in_features=11, out_features=96, bias=False)\n",
       "      (aggregation): Aggregation()\n",
       "    )\n",
       "    (1): CANLayer(\n",
       "      (lower_att): MultiHeadCellAttention(\n",
       "        (att_activation): LeakyReLU(negative_slope=0.2)\n",
       "        (lin): Linear(in_features=96, out_features=96, bias=False)\n",
       "      )\n",
       "      (upper_att): MultiHeadCellAttention(\n",
       "        (att_activation): LeakyReLU(negative_slope=0.2)\n",
       "        (lin): Linear(in_features=96, out_features=96, bias=False)\n",
       "      )\n",
       "      (lin): Linear(in_features=96, out_features=96, bias=False)\n",
       "      (aggregation): Aggregation()\n",
       "    )\n",
       "    (2): PoolLayer(\n",
       "      (signal_pool_activation): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (lin_0): Linear(in_features=96, out_features=128, bias=True)\n",
       "  (lin_1): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T09:19:41.150933630Z",
     "start_time": "2023-05-31T09:19:41.146986990Z"
    }
   },
   "outputs": [],
   "source": [
    "test_size = 0.3\n",
    "x_1_train, x_1_test = train_test_split(x_1_list, test_size=test_size, shuffle=False)\n",
    "x_0_train, x_0_test = train_test_split(x_0_list, test_size=test_size, shuffle=False)\n",
    "lower_neighborhood_train, lower_neighborhood_test = train_test_split(\n",
    "    lower_neighborhood_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "upper_neighborhood_train, upper_neighborhood_test = train_test_split(\n",
    "    upper_neighborhood_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "adjacency_0_train, adjacency_0_test = train_test_split(\n",
    "    adjacency_0_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "y_train, y_test = train_test_split(y_list, test_size=test_size, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the CCXN using low amount of epochs: we keep training minimal for the purpose of rapid testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T09:19:42.918836083Z",
     "start_time": "2023-05-31T09:19:42.114801039Z"
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::index.Tensor' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index.Tensor' is only available for these backends: [CPU, CUDA, HIP, MPS, IPU, XPU, HPU, VE, Meta, MTIA, PrivateUse1, PrivateUse2, PrivateUse3, FPGA, ORT, Vulkan, Metal, QuantizedCPU, QuantizedCUDA, QuantizedHIP, QuantizedMPS, QuantizedIPU, QuantizedXPU, QuantizedHPU, QuantizedVE, QuantizedMeta, QuantizedMTIA, QuantizedPrivateUse1, QuantizedPrivateUse2, QuantizedPrivateUse3, CustomRNGKeyId, MkldnnCPU, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nUndefined: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:31034 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:43986 [kernel]\nHIP: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nMPS: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nIPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nXPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nHPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nVE: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nMeta: registered at /dev/null:241 [kernel]\nMTIA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nPrivateUse1: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nPrivateUse2: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nPrivateUse3: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nFPGA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nORT: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nVulkan: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nMetal: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:929 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nQuantizedHIP: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nQuantizedMPS: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nQuantizedIPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nQuantizedXPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nQuantizedHPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nQuantizedVE: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nQuantizedMeta: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nQuantizedMTIA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nQuantizedPrivateUse1: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nQuantizedPrivateUse2: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nQuantizedPrivateUse3: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nCustomRNGKeyId: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nMkldnnCPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:491 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:280 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:15791 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:487 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:354 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/BatchRulesScatterOps.cpp:1179 [kernel]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1073 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:152 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:487 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m lower_neighborhood, upper_neighborhood \u001b[39m=\u001b[39m lower_neighborhood\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(\n\u001b[1;32m     20\u001b[0m     device\n\u001b[1;32m     21\u001b[0m ), upper_neighborhood\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 23\u001b[0m y_hat \u001b[39m=\u001b[39m model(x_0, x_1, adjacency, lower_neighborhood, upper_neighborhood)\n\u001b[1;32m     24\u001b[0m loss \u001b[39m=\u001b[39m crit(y_hat, y)\n\u001b[1;32m     25\u001b[0m correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (y_hat\u001b[39m.\u001b[39margmax() \u001b[39m==\u001b[39m y)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m/media/hdd/miniconda3/envs/topox/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[35], line 77\u001b[0m, in \u001b[0;36mCAN.forward\u001b[0;34m(self, x_0, x_1, neighborhood_0_to_0, lower_neighborhood, upper_neighborhood)\u001b[0m\n\u001b[1;32m     74\u001b[0m     x_1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlift_layer(x_0, neighborhood_0_to_0, x_1)\n\u001b[1;32m     76\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 77\u001b[0m     x_1 \u001b[39m=\u001b[39m layer(x_1, lower_neighborhood, upper_neighborhood)\n\u001b[1;32m     78\u001b[0m     x_1 \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(x_1, p\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[1;32m     80\u001b[0m \u001b[39m# max pooling over all nodes in each graph\u001b[39;00m\n",
      "File \u001b[0;32m/media/hdd/miniconda3/envs/topox/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/hdd/luca_s/TopoModelX/topomodelx/nn/cell/attentional_pooling_layer.py:99\u001b[0m, in \u001b[0;36mPoolLayer.forward\u001b[0;34m(self, x_0, lower_neighborhood, upper_neighborhood)\u001b[0m\n\u001b[1;32m     93\u001b[0m if self.readout:\n\u001b[1;32m     94\u001b[0m     out = scatter_add(out, top_indices, dim=0, dim_size=x_0.size(0))[\n\u001b[1;32m     95\u001b[0m         top_indices\n\u001b[1;32m     96\u001b[0m     ]\n\u001b[1;32m     98\u001b[0m # Update lower and upper neighborhood matrices with the top-k pooled edges\n\u001b[0;32m---> 99\u001b[0m # write it with torch.index_select\n\u001b[1;32m    100\u001b[0m lower_neighborhood_modified = torch.index_select(lower_neighborhood, 0, top_indices)\n\u001b[1;32m    101\u001b[0m lower_neighborhood_modified = torch.index_select(lower_neighborhood_modified, 1, top_indices)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::index.Tensor' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index.Tensor' is only available for these backends: [CPU, CUDA, HIP, MPS, IPU, XPU, HPU, VE, Meta, MTIA, PrivateUse1, PrivateUse2, PrivateUse3, FPGA, ORT, Vulkan, Metal, QuantizedCPU, QuantizedCUDA, QuantizedHIP, QuantizedMPS, QuantizedIPU, QuantizedXPU, QuantizedHPU, QuantizedVE, QuantizedMeta, QuantizedMTIA, QuantizedPrivateUse1, QuantizedPrivateUse2, QuantizedPrivateUse3, CustomRNGKeyId, MkldnnCPU, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nUndefined: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:31034 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:43986 [kernel]\nHIP: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nMPS: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nIPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nXPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nHPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nVE: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nMeta: registered at /dev/null:241 [kernel]\nMTIA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nPrivateUse1: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nPrivateUse2: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nPrivateUse3: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nFPGA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nORT: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nVulkan: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nMetal: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:929 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nQuantizedHIP: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nQuantizedMPS: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nQuantizedIPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nQuantizedXPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nQuantizedHPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nQuantizedVE: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nQuantizedMeta: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nQuantizedMTIA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nQuantizedPrivateUse1: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nQuantizedPrivateUse2: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nQuantizedPrivateUse3: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nCustomRNGKeyId: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nMkldnnCPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21456 [default backend kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:491 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:280 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:14942 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:15791 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:487 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:354 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/BatchRulesScatterOps.cpp:1179 [kernel]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1073 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:152 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:487 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "test_interval = 1\n",
    "num_epochs = 5\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    num_samples = 0\n",
    "    correct = 0\n",
    "    model.train()\n",
    "    for x_0, x_1, adjacency, lower_neighborhood, upper_neighborhood, y in zip(\n",
    "        x_0_train,\n",
    "        x_1_train,\n",
    "        adjacency_0_train,\n",
    "        lower_neighborhood_train,\n",
    "        upper_neighborhood_train,\n",
    "        y_train,\n",
    "    ):\n",
    "        x_0 = x_0.float().to(device)\n",
    "        x_1, y = x_1.float().to(device), torch.tensor(y, dtype=torch.long).to(device)\n",
    "        adjacency = adjacency.float().to(device)\n",
    "        lower_neighborhood, upper_neighborhood = lower_neighborhood.float().to(\n",
    "            device\n",
    "        ), upper_neighborhood.float().to(device)\n",
    "        opt.zero_grad()\n",
    "        y_hat = model(x_0, x_1, adjacency, lower_neighborhood, upper_neighborhood)\n",
    "        loss = crit(y_hat, y)\n",
    "        correct += (y_hat.argmax() == y).sum().item()\n",
    "        num_samples += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "    train_acc = correct / num_samples\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f} Train_acc: {train_acc:.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            num_samples = 0\n",
    "            correct = 0\n",
    "            for x_0, x_1, adjacency, lower_neighborhood, upper_neighborhood, y in zip(\n",
    "                x_0_test,\n",
    "                x_1_test,\n",
    "                adjacency_0_test,\n",
    "                lower_neighborhood_test,\n",
    "                upper_neighborhood_test,\n",
    "                y_test,\n",
    "            ):\n",
    "                x_0 = x_0.float().to(device)\n",
    "                x_1, y = x_1.float().to(device), torch.tensor(y, dtype=torch.long).to(\n",
    "                    device\n",
    "                )\n",
    "                adjacency = adjacency.float().to(device)\n",
    "                lower_neighborhood, upper_neighborhood = lower_neighborhood.float().to(\n",
    "                    device\n",
    "                ), upper_neighborhood.float().to(device)\n",
    "                y_hat = model(\n",
    "                    x_0, x_1, adjacency, lower_neighborhood, upper_neighborhood\n",
    "                )\n",
    "                correct += (y_hat.argmax() == y).sum().item()\n",
    "                num_samples += 1\n",
    "            test_acc = correct / num_samples\n",
    "            print(f\"Test_acc: {test_acc:.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('topox')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b696409e3d9b84bb97012e0a2d03075417bfa260eb8ad887be094cb925d5d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
